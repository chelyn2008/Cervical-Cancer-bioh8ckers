{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866bb08f",
   "metadata": {},
   "source": [
    "### DATA PREDICTION MODEL: Cervical Cancer Binary Classification\n",
    "\n",
    "##### Workflow:\n",
    "1. Load preprocessed data\n",
    "2. Split into training/testing sets\n",
    "3. Scale features\n",
    "4. Train multiple classification models\n",
    "5. Evaluate models using metrics like Accuracy, Precision, Recall, F1 Score, AUC\n",
    "6. Visualize confusion matrices and ROC curves\n",
    "7. Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad844061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f14637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TO READ FILE\"\"\"\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\Deonne\\OneDrive - Nanyang Technological University\\Desktop\\Y2S1\\Biohackathon\")\n",
    "#os.chdir(r\"path\")\n",
    "print(\"Current working directory is:\", os.getcwd())\n",
    "\n",
    "#load cleaned data\n",
    "clean_data = pd.read_csv('cleaned_cervical_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target variable\n",
    "target = 'Dx:Cancer'\n",
    "X = clean_data.drop(columns=[target])\n",
    "Y = clean_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186096a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and testing sets (80-20)\n",
    "X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Standardise features (important for SVM and Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_training_scaled = scaler.fit_transform(X_training)\n",
    "X_testing_scaled = scaler.transform(X_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c0662",
   "metadata": {},
   "source": [
    "### Possible MODELS:\n",
    "- Logistic Regression (linear classifier, interpretable)\n",
    "- Random Forest (ensemble method, handles non-linearity well)\n",
    "- Support Vector Machine (robust with high-dimensional data)\n",
    "- XGBoost (gradient boosting, powerful for structured data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d052ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = {\n",
    "    \"Logistics Regression\": LogisticRegression(class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced',random_state=42),\n",
    "    \"SVM\": SVC(probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b108e",
   "metadata": {},
   "source": [
    "### TRAIN & EVALUATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ac931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train + evaluate models\n",
    "result = []\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_training_scaled, Y_training)\n",
    "    fitted_models[name] = model\n",
    "\n",
    "    Y_prediction = model.predict(X_testing_scaled)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        Y_probability = model.predict_proba(X_testing_scaled)[:, 1]\n",
    "    else:\n",
    "        Y_probability = None\n",
    "\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_testing, Y_prediction):.4f}\")\n",
    "    print(f\"Precision: {precision_score(Y_testing, Y_prediction):.4f}\")\n",
    "    print(f\"Recall: {recall_score(Y_testing, Y_prediction):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(Y_testing, Y_prediction):.4f}\")\n",
    "\n",
    "    print(classification_report(Y_testing, Y_prediction))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b17e92",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "It shows how many True Positive, True Negatives, False Positive, True Positive for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for name, model in fitted_models.items():\n",
    "    Y_predicted = model.predict(X_testing_scaled)\n",
    "    confusion_mtx = confusion_matrix(Y_testing, Y_predicted)\n",
    "    \n",
    "    #plot as heatmap for visualisation\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(\n",
    "        confusion_mtx, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap=\"Blues\", \n",
    "        xticklabels=['Cancer negative', 'Cancer positive'], \n",
    "        yticklabels=['Cancer negative', 'Cancer positive']\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b1a58",
   "metadata": {},
   "source": [
    "### ROC Curve & AUC\n",
    "\n",
    "ROC Curve: shows trade-off between TP rate(Recall) and FP rate\n",
    "\n",
    "AUC (Area Under Curve): indicates how well a model can distinguish between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7005c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_testing_scaled)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(Y_testing, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f\"ROC Curve: {name}\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644023b",
   "metadata": {},
   "source": [
    "### Model Performances\n",
    "summerisation of all the evaluation metrics (Accuracy, Precision, Recall, F1 Score, AUC) in a DataFrame for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "metrics_summary = []\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    y_pred = model.predict(X_testing_scaled)\n",
    "    y_prob = model.predict_proba(X_testing_scaled)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics_summary.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(Y_testing, y_pred),\n",
    "        \"Precision\": precision_score(Y_testing, y_pred),\n",
    "        \"Recall\": recall_score(Y_testing, y_pred),\n",
    "        \"F1 Score\": f1_score(Y_testing, y_pred),\n",
    "        \"AUC\": roc_auc_score(Y_testing, y_prob) if y_prob is not None else None\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(metrics_summary)\n",
    "results_df = results_df.sort_values(by=\"Recall\", ascending=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f764ae7",
   "metadata": {},
   "source": [
    "### Model Comparisons:\n",
    "\n",
    "Key evaluation metrics:\n",
    "- **Recall (TP rate)** -- Avoid false negatives\n",
    "- **Precision** -- Avoid false positives\n",
    "- **F1 Score** -- Balance between Precision & Recall\n",
    "- **AUC** -- overall classification power at all thresholds\n",
    "\n",
    "\n",
    "#### Observations:\n",
    "- **Logistic Regression**: Shows perfect metrics\n",
    "- **Random Forest**: High precision but lower recall (misses ~â…“ cancer cases)\n",
    "- **SVM/XGBoost**: Trade-off between metrics, worth hyperparameter tuning\n",
    "\n",
    "##### Conclusion: Logistics Regression appears most promising\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
